# Major Q1 Extended

## Introduction  
This notebook demonstrates the implementation of three regression algorithms: Ridge Regression, Support Vector Machine (SVM) Regression, and XGBoost Regression. We will conduct hyperparameter tuning using GridSearchCV and evaluate model performance using multiple metrics.

## Import Libraries  
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
```

## Data Loading  
```python
# Load the dataset
data = pd.read_csv('your_dataset.csv')
```

## Data Preprocessing  
```python
# Preprocess the data
# Handle missing values, encoding categorical variables, etc.

data.fillna(method='ffill', inplace=True)
X = data.drop('target_variable', axis=1)
y = data['target_variable']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

## Model Training  
### Ridge Regression  
```python
ridge = Ridge()
params = {'alpha': [0.1, 1.0, 10.0]}
grid_ridge = GridSearchCV(ridge, params, cv=5)
grid_ridge.fit(X_train, y_train)
```

### SVM Regression  
```python
svm = SVR()
params_svm = {'C': [0.1, 1, 10], 'epsilon': [0.01, 0.1, 0.2]}
grid_svm = GridSearchCV(svm, params_svm, cv=5)
grid_svm.fit(X_train, y_train)
```

### XGBoost Regression  
```python
xgb = XGBRegressor()
params_xgb = {'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2]}
grid_xgb = GridSearchCV(xgb, params_xgb, cv=5)
grid_xgb.fit(X_train, y_train)
```

## Evaluation Metrics  
### Ridge Regression  
```python
ridge_preds = grid_ridge.predict(X_test)
mae_ridge = mean_absolute_error(y_test, ridge_preds)
mse_ridge = mean_squared_error(y_test, ridge_preds)
rmse_ridge = np.sqrt(mse_ridge)
```

### SVM Regression  
```python
svm_preds = grid_svm.predict(X_test)
mae_svm = mean_absolute_error(y_test, svm_preds)
mse_svm = mean_squared_error(y_test, svm_preds)
rmse_svm = np.sqrt(mse_svm)
```

### XGBoost Regression  
```python
xgb_preds = grid_xgb.predict(X_test)
mae_xgb = mean_absolute_error(y_test, xgb_preds)
mse_xgb = mean_squared_error(y_test, xgb_preds)
rmse_xgb = np.sqrt(mse_xgb)
```

## Cross-Validation Scores  
```python
cv_ridge = cross_val_score(grid_ridge, X, y, cv=5).mean()
cv_svm = cross_val_score(grid_svm, X, y, cv=5).mean()
cv_xgb = cross_val_score(grid_xgb, X, y, cv=5).mean()
```

## Model Comparison Table  
```python
comparison = pd.DataFrame({
    'Model': ['Ridge', 'SVM', 'XGBoost'],
    'MAE': [mae_ridge, mae_svm, mae_xgb],
    'MSE': [mse_ridge, mse_svm, mse_xgb],
    'RMSE': [rmse_ridge, rmse_svm, rmse_xgb],
    'CV Score': [cv_ridge, cv_svm, cv_xgb]
})
comparison
```

## Visualizations  
### Model Performance Comparison  
```python
plt.figure(figsize=(12, 6))
sns.barplot(x='Model', y='MAE', data=comparison)
plt.title('Model Performance Comparison - MAE')
plt.show()
```

### Feature Importance for XGBoost  
```python
xgb_feature_importance = grid_xgb.best_estimator_.feature_importances_
plt.figure(figsize=(10, 5))
sns.barplot(x=xgb_feature_importance, y=X.columns)
plt.title('Feature Importance for XGBoost')
plt.show()
```

## Conclusion  
This notebook covered the implementation of Ridge Regression, SVM Regression, and XGBoost Regression. We performed hyperparameter tuning, evaluated models based on various metrics, and visualized the results.